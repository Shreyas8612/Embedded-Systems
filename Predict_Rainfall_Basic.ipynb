{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:43.446625Z",
     "start_time": "2025-03-21T13:07:43.429696Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from jedi.inference import InferenceState\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from queue import Queue\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "File_Path = '/Users/shreyasravi/PycharmProjects/Embedded-Systems/London_Weather.csv'"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A thread-safe class for preprocessing weather data, implementing feature selection, and preparing data chunks for LSTM training and inference processes. This class acts as a producer in a producer-consumer pattern, sending data to both training and inference threads.\n",
    "\n",
    "Semaphore is like a counter which is used to control the number of threads that can access a shared resource at the same time. It is a signaling mechanism like traffic light block threads when resources are not available and allow threads when the resources are available."
   ],
   "id": "d2c705c3fe36735c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the LSTM, there are three key gates:\n",
    "\n",
    "1. The Forget Gate manages what should be forgotten.\n",
    "2. The Input Gate manages what should be kept, and\n",
    "3. The Output Gate manages what information is stored in the carry and hidden states.\n",
    "\n",
    "Each of these gates comprise of their own neural network layer that handles the mathematics needed to retain the relevant data we need to store in both the short-term memory and long-term memory."
   ],
   "id": "8166177581b8cff8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This class acts as a consumer in the producer-consumer pattern,\n",
    "consuming data from the DataPreprocessing class and training an LSTM model."
   ],
   "id": "b7651ff5ea550d8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Thread-safe class for inferring using a trained LSTM model for weather forecasting.\n",
    "\n",
    "This class acts as a consumer in a producer-consumer pattern, consuming test data\n",
    "from the preprocessing thread and generating predictions using the trained model."
   ],
   "id": "5fb930811a4ed6b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:43.474250Z",
     "start_time": "2025-03-21T13:07:43.467043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "        self.scaler = None\n",
    "        self.normalized_data = None\n",
    "        self.train_size = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.T = 20  # Number of timesteps to look while predicting\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        return self.data\n",
    "\n",
    "    def clean_data(self):\n",
    "        # Convert date to datetime format\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'], format='%Y%m%d')\n",
    "\n",
    "        # Fill missing values in snow_depth with 0\n",
    "        self.data['snow_depth'].fillna(0, inplace=True)\n",
    "\n",
    "        # Drop rows with any NaN values\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def prepare_data_for_model(self, train_ratio=0.8, target_column='mean_temp'):\n",
    "        # Separate features and target\n",
    "        input_data = self.data.drop(['date'], axis=1)\n",
    "        targets = self.data[target_column].values\n",
    "\n",
    "        # Define dimensions\n",
    "        D = input_data.shape[1]  # Number of features\n",
    "        N = len(input_data) - self.T\n",
    "\n",
    "        # Calculate train size\n",
    "        self.train_size = int(len(input_data) * train_ratio)\n",
    "\n",
    "        # Normalize input data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(input_data[:self.train_size + self.T - 1])\n",
    "        self.normalized_data = self.scaler.transform(input_data)\n",
    "\n",
    "        # Prepare X_train and y_train\n",
    "        self.X_train = np.zeros((self.train_size, self.T, D))\n",
    "        self.y_train = np.zeros((self.train_size, 1))\n",
    "\n",
    "        for t in range(self.train_size):\n",
    "            self.X_train[t, :, :] = self.normalized_data[t:t+self.T]\n",
    "            self.y_train[t] = targets[t+self.T]\n",
    "\n",
    "        # Prepare X_test and y_test\n",
    "        self.X_test = np.zeros((N - self.train_size, self.T, D))\n",
    "        self.y_test = np.zeros((N - self.train_size, 1))\n",
    "\n",
    "        for i in range(N - self.train_size):\n",
    "            t = i + self.train_size\n",
    "            self.X_test[i, :, :] = self.normalized_data[t:t+self.T]\n",
    "            self.y_test[i] = targets[t+self.T]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        self.X_train = torch.from_numpy(self.X_train.astype(np.float32))\n",
    "        self.y_train = torch.from_numpy(self.y_train.astype(np.float32))\n",
    "        self.X_test = torch.from_numpy(self.X_test.astype(np.float32))\n",
    "        self.y_test = torch.from_numpy(self.y_test.astype(np.float32))\n",
    "\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "    def get_train_test_data(self):\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "    def get_original_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n"
   ],
   "id": "e3e01f49def956a7",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:44.149169Z",
     "start_time": "2025-03-21T13:07:44.145995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.M = hidden_dim\n",
    "        self.L = layer_dim\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=layer_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # batch_first to have (batch_dim, seq_dim, feature_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Set device - ideally this would be a class property set during initialization\n",
    "        device = X.device\n",
    "\n",
    "        # Initial hidden state and cell state\n",
    "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "\n",
    "        out, (hn, cn) = self.rnn(X, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # h(T) at the final time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "id": "66411eb210407062",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:44.448643Z",
     "start_time": "2025-03-21T13:07:44.443040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Training:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_losses = None\n",
    "        self.test_losses = None\n",
    "\n",
    "    def create_model(self, input_dim, hidden_dim=512, layer_dim=2, output_dim=1):\n",
    "        self.model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "        self.model.to(self.device)\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_test, y_test, learning_rate=0.01, epochs=200):\n",
    "        # Move data to device\n",
    "        X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n",
    "        X_test, y_test = X_test.to(self.device), y_test.to(self.device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        self.train_losses = np.zeros(epochs)\n",
    "        self.test_losses = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Train loss\n",
    "            self.train_losses[epoch] = loss.item()\n",
    "\n",
    "            # Test loss\n",
    "            test_outputs = self.model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            self.test_losses[epoch] = test_loss.item()\n",
    "\n",
    "        return self.train_losses, self.test_losses\n",
    "\n",
    "    def load_model(self, model_path=None, weights_path=None):\n",
    "        if model_path is not None:\n",
    "            self.model = torch.load(model_path, map_location=self.device)\n",
    "        elif weights_path is not None and self.model is not None:\n",
    "            self.model.load_state_dict(torch.load(weights_path, map_location=self.device))\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def get_losses(self):\n",
    "        return self.train_losses, self.test_losses"
   ],
   "id": "af8b96504353fedd",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:44.453609Z",
     "start_time": "2025-03-21T13:07:44.450660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Inference:\n",
    "    def __init__(self, model=None, device=None):\n",
    "        self.model = model\n",
    "        self.device = device if device is not None else torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.predictions = None\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Move data to device\n",
    "        X_test = X_test.to(self.device)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Generate predictions\n",
    "        self.predictions = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(X_test)):\n",
    "                input_ = X_test[i].reshape(1, X_test.shape[1], X_test.shape[2])\n",
    "                p = self.model(input_)[0, 0].item()\n",
    "                self.predictions.append(p)\n",
    "        return self.predictions\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return self.predictions"
   ],
   "id": "ab8a77d5ffaed618",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:44.606449Z",
     "start_time": "2025-03-21T13:07:44.602261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Analysis:\n",
    "    def __init__(self):\n",
    "        self.true_values = None\n",
    "        self.predictions = None\n",
    "        self.metrics = {}\n",
    "\n",
    "    def set_data(self, true_values, predictions):\n",
    "        if isinstance(true_values, torch.Tensor):\n",
    "            self.true_values = true_values.cpu().detach().numpy()\n",
    "        else:\n",
    "            self.true_values = true_values\n",
    "\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        if self.true_values is None or self.predictions is None:\n",
    "            raise ValueError(\"Data not set. Call set_data first.\")\n",
    "\n",
    "        # Calculate MAE\n",
    "        self.metrics['MAE'] = mean_absolute_error(self.true_values, self.predictions)\n",
    "\n",
    "        # Calculate MSE\n",
    "        self.metrics['MSE'] = mean_squared_error(self.true_values, self.predictions)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        self.metrics['RMSE'] = np.sqrt(self.metrics['MSE'])\n",
    "\n",
    "        print(f\"Metrics calculated - MAE: {self.metrics['MAE']:.3f}, MSE: {self.metrics['MSE']:.3f}, RMSE: {self.metrics['RMSE']:.3f}\")\n",
    "        return self.metrics\n",
    "\n",
    "    def analyze_error_distribution(self):\n",
    "        # Calculate errors\n",
    "        errors = self.true_values.flatten() - np.array(self.predictions)\n",
    "\n",
    "        # Basic statistics of errors\n",
    "        error_stats = {\n",
    "            'mean': np.mean(errors),\n",
    "            'std': np.std(errors),\n",
    "            'min': np.min(errors),\n",
    "            'max': np.max(errors)\n",
    "        }\n",
    "\n",
    "        print(f\"Error statistics - Mean: {error_stats['mean']:.3f}, Std: {error_stats['std']:.3f}, Min: {error_stats['min']:.3f}, Max: {error_stats['max']:.3f}\")\n",
    "        return errors, error_stats\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.metrics"
   ],
   "id": "4d9579f3742615db",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:07:44.759169Z",
     "start_time": "2025-03-21T13:07:44.753838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Display:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.predictions = None\n",
    "        self.dates = None\n",
    "\n",
    "    def set_data(self, original_data, predictions, start_idx=None):\n",
    "        self.data = original_data\n",
    "        self.predictions = predictions\n",
    "\n",
    "        # Prepare the plot DataFrame\n",
    "        plot_len = len(predictions)\n",
    "        if start_idx is None:\n",
    "            start_idx = -plot_len\n",
    "\n",
    "        self.plot_df = original_data[['date', 'mean_temp']].copy(deep=True)\n",
    "        self.plot_df = self.plot_df.iloc[start_idx:]\n",
    "        self.plot_df['prediction'] = predictions\n",
    "        self.plot_df.set_index('date', inplace=True)\n",
    "\n",
    "    def plot_results(self, title=None, figsize=(20, 10)):\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(self.plot_df['mean_temp'], label='Actual Temperature', linewidth=1)\n",
    "        plt.plot(self.plot_df['prediction'], label='Predicted Temperature', linewidth=1)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Temperature (°C)')\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_by_year(self, figsize=(20, 10)):\n",
    "        # Group data by year\n",
    "        plot_df_by_years = []\n",
    "        for y in self.plot_df.index.year.unique():\n",
    "            plot_df_by_years.append((y, self.plot_df.loc[self.plot_df.index.year == y]))\n",
    "\n",
    "        # Plot each year separately\n",
    "        for year, year_df in plot_df_by_years:\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.plot(year_df['mean_temp'], label='Actual Temperature', linewidth=1)\n",
    "            plt.plot(year_df['prediction'], label='Predicted Temperature', linewidth=1)\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Temperature (°C)')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.title(f'Temperature in {year}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def plot_error_histogram(self, errors, bins=25, figsize=(12, 8)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.hist(errors, bins=bins)\n",
    "        plt.xlabel('Temperature Difference (Actual - Predicted)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Prediction Errors')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_training_history(self, train_losses, test_losses, figsize=(12, 8)):\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training History')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "34543dadb667ec3b",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:08:04.700539Z",
     "start_time": "2025-03-21T13:07:45.060532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants and configurations\n",
    "FILE_PATH = 'London_Weather.csv'  # Update with your file path\n",
    "CAPACITY = 15  # Size of data buffer\n",
    "DATALOADER_LOAD_THRESHOLD = 1  # How many data points to load at once\n",
    "TRAINER_READ_THRESHOLD = 3  # How many data points trainer reads at once\n",
    "INFERENCE_READ_THRESHOLD = 1  # How many data points inference reads at once\n",
    "\n",
    "# Wait times to simulate processing time\n",
    "DATALOADER_WAIT_TIME = 1  # seconds\n",
    "TRAINER_WAIT_TIME = 5  # seconds\n",
    "INFERENCE_WAIT_TIME = 2  # seconds\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 70\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Create shared data buffer for holding processed batches\n",
    "data_buffer = [None for _ in range(CAPACITY)]\n",
    "main_start_time = time.time()\n",
    "preprocess_start_time = time.time()\n",
    "\n",
    "# Initialize the preprocessing class\n",
    "preprocessor = Preprocessing(FILE_PATH)\n",
    "preprocessor.load_data()\n",
    "preprocessor.clean_data()\n",
    "\n",
    "# Prepare data for model\n",
    "X_train, y_train, X_test, y_test = preprocessor.prepare_data_for_model(target_column='mean_temp')\n",
    "\n",
    "# Fill the buffer with batches of data\n",
    "datapoints_loaded = 0\n",
    "in_index = 0\n",
    "\n",
    "# We'll create mini-batches from the training data to fill the buffer\n",
    "batch_size = max(1, len(X_train) // CAPACITY)  # Ensure at least 1 example per batch\n",
    "\n",
    "while datapoints_loaded < CAPACITY:\n",
    "    # Create a batch of data\n",
    "    start_idx = datapoints_loaded * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(X_train))\n",
    "\n",
    "    if start_idx >= len(X_train):\n",
    "        break\n",
    "\n",
    "    batch_X = X_train[start_idx:end_idx]\n",
    "    batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "    # Store batch in buffer\n",
    "    data_buffer[in_index] = (batch_X, batch_y)\n",
    "\n",
    "    print(f\"[DATA LOADER] Added batch {datapoints_loaded+1} to buffer position {in_index+1}, size: {len(batch_X)}\")\n",
    "\n",
    "    # Simulate processing time\n",
    "    time.sleep(DATALOADER_WAIT_TIME)\n",
    "\n",
    "    # Update indices and counters\n",
    "    in_index = (in_index + DATALOADER_LOAD_THRESHOLD) % CAPACITY\n",
    "    datapoints_loaded += DATALOADER_LOAD_THRESHOLD\n",
    "\n",
    "preprocess_end_time = time.time()\n",
    "print(f\"[DATA LOADER] Completed in {preprocess_end_time - preprocess_start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Initialize trainer and create model\n",
    "trainer = Training()\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "model = trainer.create_model(input_dim=input_dim)\n",
    "\n",
    "# Read batches from the buffer and train\n",
    "read_datapoints_in_trainer = 0\n",
    "out_index_trainer = 0\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "while read_datapoints_in_trainer < CAPACITY:\n",
    "    # Check if we've reached the end of useful data\n",
    "    if out_index_trainer >= datapoints_loaded:\n",
    "        break\n",
    "\n",
    "    # Get batches from buffer (up to 3 at a time)\n",
    "    trainer_batches_X = []\n",
    "    trainer_batches_y = []\n",
    "\n",
    "    for i in range(out_index_trainer, min(out_index_trainer + TRAINER_READ_THRESHOLD, datapoints_loaded)):\n",
    "        batch_X, batch_y = data_buffer[i]\n",
    "        trainer_batches_X.append(batch_X)\n",
    "        trainer_batches_y.append(batch_y)\n",
    "\n",
    "    # Combine batches\n",
    "    combined_X = torch.cat(trainer_batches_X)\n",
    "    combined_y = torch.cat(trainer_batches_y)\n",
    "\n",
    "    print(f\"[TRAINER] Processing batches from positions {out_index_trainer+1} to {min(out_index_trainer+TRAINER_READ_THRESHOLD, datapoints_loaded)}\")\n",
    "\n",
    "    # Train on this combined batch for a few epochs\n",
    "    mini_epochs = 5  # Train for 5 epochs on each batch\n",
    "    for epoch in range(mini_epochs):\n",
    "        # Move data to device\n",
    "        combined_X = combined_X.to(device)\n",
    "        combined_y = combined_y.to(device)\n",
    "        X_test_sample = X_test[:min(len(X_test), 100)].to(device)  # Use a subset for validation\n",
    "        y_test_sample = y_test[:min(len(y_test), 100)].to(device)\n",
    "\n",
    "        # Train for one epoch\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(combined_X)\n",
    "        loss = criterion(outputs, combined_y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute validation loss\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_sample)\n",
    "            test_loss = criterion(test_outputs, y_test_sample)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f\"[TRAINER] Epoch {epoch+1}/{mini_epochs}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "    # Simulate processing time\n",
    "    time.sleep(TRAINER_WAIT_TIME)\n",
    "\n",
    "    # Update indices and counters\n",
    "    out_index_trainer = (out_index_trainer + TRAINER_READ_THRESHOLD) % CAPACITY\n",
    "    read_datapoints_in_trainer += TRAINER_READ_THRESHOLD\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"[TRAINER] Completed in {training_end_time - training_start_time:.2f} seconds\")\n",
    "\n",
    "inference_start_time = time.time()\n",
    "\n",
    "# Initialize inference\n",
    "inference = Inference()\n",
    "inference.set_model(model)\n",
    "\n",
    "# Process test data one by one\n",
    "read_datapoints_in_inferer = 0\n",
    "out_index_inferer = 0\n",
    "all_predictions = []\n",
    "\n",
    "while read_datapoints_in_inferer < len(X_test):\n",
    "    if read_datapoints_in_inferer >= len(X_test):\n",
    "        break\n",
    "\n",
    "    # Get one test example\n",
    "    test_sample = X_test[read_datapoints_in_inferer].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = inference.predict(test_sample)[0]  # Get the first (and only) prediction\n",
    "    all_predictions.append(prediction)\n",
    "\n",
    "    # Simulate processing time\n",
    "    time.sleep(INFERENCE_WAIT_TIME)\n",
    "\n",
    "    # Update indices and counters\n",
    "    read_datapoints_in_inferer += INFERENCE_READ_THRESHOLD\n",
    "\n",
    "    # Only process a subset of the test data for demonstration\n",
    "    if read_datapoints_in_inferer >= CAPACITY:\n",
    "        break\n",
    "\n",
    "inference_end_time = time.time()\n",
    "print(f\"[INFERENCE] Completed in {inference_end_time - inference_start_time:.2f} seconds\")\n",
    "\n",
    "analysis_start_time = time.time()\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions = np.array(all_predictions)\n",
    "\n",
    "# Analyze model performance\n",
    "analyzer = Analysis()\n",
    "analyzer.set_data(y_test[:len(predictions)], predictions)\n",
    "metrics = analyzer.calculate_metrics()\n",
    "errors, error_stats = analyzer.analyze_error_distribution()\n",
    "\n",
    "analysis_end_time = time.time()\n",
    "print(f\"[ANALYSIS] Completed in {analysis_end_time - analysis_start_time:.2f} seconds\")\n",
    "\n",
    "display_start_time = time.time()\n",
    "\n",
    "# Visualize results\n",
    "displayer = Display()\n",
    "original_data = preprocessor.get_original_data()\n",
    "displayer.set_data(original_data, predictions, start_idx=-len(predictions))\n",
    "\n",
    "# Plot results\n",
    "displayer.plot_results(title='LSTM Temperature Forecast')\n",
    "\n",
    "# Plot by year\n",
    "displayer.plot_by_year()\n",
    "\n",
    "# Plot error histogram\n",
    "displayer.plot_error_histogram(errors)\n",
    "\n",
    "# Plot training history\n",
    "displayer.plot_training_history(train_losses, test_losses)\n",
    "\n",
    "display_end_time = time.time()\n",
    "print(f\"[DISPLAY] Completed in {display_end_time - display_start_time:.2f} seconds\")\n",
    "\n",
    "main_end_time = time.time()\n",
    "print(f'[CONSOLE] Total operation time taken is {main_end_time - main_start_time:.2f} seconds')\n",
    "\n",
    "# Print summary of metrics\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ],
   "id": "23fddf201bcbc4cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA LOADER] Added batch 1 to buffer position 1, size: 813\n",
      "[DATA LOADER] Added batch 2 to buffer position 2, size: 813\n",
      "[DATA LOADER] Added batch 3 to buffer position 3, size: 813\n",
      "[DATA LOADER] Added batch 4 to buffer position 4, size: 813\n",
      "[DATA LOADER] Added batch 5 to buffer position 5, size: 813\n",
      "[DATA LOADER] Added batch 6 to buffer position 6, size: 813\n",
      "[DATA LOADER] Added batch 7 to buffer position 7, size: 813\n",
      "[DATA LOADER] Added batch 8 to buffer position 8, size: 813\n",
      "[DATA LOADER] Added batch 9 to buffer position 9, size: 813\n",
      "[DATA LOADER] Added batch 10 to buffer position 10, size: 813\n",
      "[DATA LOADER] Added batch 11 to buffer position 11, size: 813\n",
      "[DATA LOADER] Added batch 12 to buffer position 12, size: 813\n",
      "[DATA LOADER] Added batch 13 to buffer position 13, size: 813\n",
      "[DATA LOADER] Added batch 14 to buffer position 14, size: 813\n",
      "[DATA LOADER] Added batch 15 to buffer position 15, size: 813\n",
      "[DATA LOADER] Completed in 15.17 seconds\n",
      "[TRAINER] Processing batches from positions 1 to 3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 112\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m    111\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 112\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_X\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, combined_y)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[107], line 24\u001B[0m, in \u001B[0;36mLSTMModel.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     21\u001B[0m h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mL, X\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     22\u001B[0m c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mL, X\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 24\u001B[0m out, (hn, cn) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh0\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc0\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# h(T) at the final time step\u001B[39;00m\n\u001B[1;32m     27\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :])\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m   1121\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1124\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1125\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1127\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1128\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1129\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1130\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1133\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1134\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1136\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[1;32m   1137\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   1138\u001B[0m         batch_sizes,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1145\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[1;32m   1146\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "execution_count": 112
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

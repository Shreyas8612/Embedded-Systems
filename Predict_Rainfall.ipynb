{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T20:34:41.597536Z",
     "start_time": "2025-03-10T20:34:41.580736Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from queue import Queue\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "File_Path = '/Users/shreyasravi/PycharmProjects/Embedded-Systems/London_Weather.csv'"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A thread-safe class for preprocessing weather data, implementing feature selection, and preparing data chunks for LSTM training and inference processes. This class acts as a producer in a producer-consumer pattern, sending data to both training and inference threads.\n",
    "\n",
    "Perform Correlation-based Feature Selection (CFS) by thresholding to determine relevant features\n",
    "\n",
    "Semaphore is like a counter which is used to control the number of threads that can access a shared resource at the same time. It is a signaling mechanism like traffic light block threads when resources are not available and allow threads when the resources are available."
   ],
   "id": "d2c705c3fe36735c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T20:39:52.279937Z",
     "start_time": "2025-03-10T20:39:52.259819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataPreprocessing(threading.Thread):\n",
    "    def __init__(self,\n",
    "                 data_path: str,  # Path to the weather dataset CSV\n",
    "                 time_steps: int = 20,  # Number of timesteps to look at while predicting\n",
    "                 train_size: float = 0.8,  # Proportion of data to use for training - 80%\n",
    "                 trainer_chunk_size: int = 10,  # Number of data chucks to send to the trainer\n",
    "                 inference_chunk_size: int = 1,  # Number of data chunks to send to the inference\n",
    "                 capacity: int = 5,  # Buffer capacity for each consumer\n",
    "                 target_column: str = 'mean_temp'):  # Column to predict (e.g. 'mean_temp' or 'precipitation')\n",
    "\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        # Data parameters\n",
    "        self.data_path = data_path\n",
    "        self.time_steps = time_steps\n",
    "        self.train_size = train_size\n",
    "        self.trainer_chunk_size = trainer_chunk_size\n",
    "        self.inference_chunk_size = inference_chunk_size\n",
    "        self.target_column = target_column\n",
    "\n",
    "        # Thread synchronization primitives\n",
    "        self.capacity = capacity\n",
    "        self.trainer_buffer = Queue(maxsize=capacity)\n",
    "        self.inference_buffer = Queue(maxsize=capacity)\n",
    "\n",
    "        # Semaphores for trainer\n",
    "        self.trainer_mutex = threading.Semaphore()  # It is like ON and OFF switch and only one thread can access the trainer\n",
    "        self.trainer_empty = threading.Semaphore(capacity)  # It has a capacity of 5, so 5 threads can access the trainer\n",
    "        self.trainer_full = threading.Semaphore(0)  # It is like a counter, it is 0, so no thread can access the trainer\n",
    "\n",
    "        # Semaphores for inference\n",
    "        self.inference_mutex = threading.Semaphore()  # It is like ON and OFF switch and only one thread can access the trainer\n",
    "        self.inference_empty = threading.Semaphore(capacity)  # It has a capacity of 5, so 5 threads can access the trainer\n",
    "        self.inference_full = threading.Semaphore(0)  # It is like a counter, it is 0, so no thread can access the trainer\n",
    "\n",
    "        # Internal state\n",
    "        self.data = None\n",
    "        self.features = None\n",
    "        self.scaler = None\n",
    "        self.features_selected = False\n",
    "        self.data_processed = False\n",
    "        self.relevant_features = []\n",
    "        self.irrelevant_features = []\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "        # Tracking variables\n",
    "        self.trainer_chunks_sent = 0\n",
    "        self.inference_chunks_sent = 0\n",
    "        self.is_running = True\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        # Load and perform basic cleaning of the data\n",
    "        print(f\"Loading data from {self.data_path}\")\n",
    "        self.data = pd.read_csv(self.data_path)\n",
    "\n",
    "        # Basic cleaning and preprocessing\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'], format='%Y%m%d')\n",
    "        self.data['snow_depth'] = self.data['snow_depth'].fillna(0)\n",
    "\n",
    "        # Drop rows with any remaining NaN values\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        # Extract features (All columns except date and Target)\n",
    "        self.features = self.data.drop(['date', self.target_column], axis=1)\n",
    "        self.targets = self.data[self.target_column].values\n",
    "\n",
    "        print(f\"Data loaded: {len(self.data)} rows with {len(self.features.columns)} features\")\n",
    "\n",
    "    def select_features(self, correlation_threshold: float = 0.4) -> None:\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Data must be loaded before feature selection\")\n",
    "\n",
    "        print(\"Performing Correlation-based Feature Selection (CFS)\")\n",
    "\n",
    "        # Calculate correlation with target\n",
    "        correlations = {}\n",
    "        target_values = self.data[self.target_column]\n",
    "\n",
    "        for column in self.features.columns:\n",
    "            correlation = abs(self.data[column].corr(target_values))\n",
    "            correlations[column] = correlation\n",
    "\n",
    "        # Categorize features\n",
    "        self.relevant_features = []\n",
    "        self.irrelevant_features = []\n",
    "\n",
    "        for feature, correlation in correlations.items():\n",
    "            if correlation >= correlation_threshold:\n",
    "                self.relevant_features.append(feature)\n",
    "            else:\n",
    "                self.irrelevant_features.append(feature)\n",
    "\n",
    "        print(f\"Selected {len(self.relevant_features)} relevant features: {self.relevant_features}\")\n",
    "        print(f\"Identified {len(self.irrelevant_features)} less relevant features: {self.irrelevant_features}\")\n",
    "\n",
    "        # Update features to only include relevant ones\n",
    "        self.features = self.features[self.relevant_features]\n",
    "        self.features_selected = True\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        if not self.features_selected:\n",
    "            print(\"Warning: Feature selection has not been performed\")\n",
    "\n",
    "        print(\"Preparing data for LSTM training and inference\")\n",
    "\n",
    "        # Get dimensions\n",
    "        D = self.features.shape[1]  # Number of features\n",
    "        N = len(self.features) - self.time_steps\n",
    "\n",
    "        # Split into train and test\n",
    "        train_size = int(len(self.features) * self.train_size)\n",
    "\n",
    "        # Normalize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.features[:train_size])\n",
    "        features_normalized = self.scaler.transform(self.features)\n",
    "\n",
    "        # Preparing X_train and y_train\n",
    "        self.X_train = np.zeros((train_size, self.time_steps, D))\n",
    "        self.y_train = np.zeros((train_size, 1))\n",
    "\n",
    "        for t in range(train_size):\n",
    "            self.X_train[t, :, :] = features_normalized[t:t + self.time_steps]\n",
    "            self.y_train[t] = self.targets[t + self.time_steps]\n",
    "\n",
    "        # Preparing X_test and y_test\n",
    "        self.X_test = np.zeros((N - train_size, self.time_steps, D))\n",
    "        self.y_test = np.zeros((N - train_size, 1))\n",
    "\n",
    "        for i in range(N - train_size):\n",
    "            t = i + train_size\n",
    "            self.X_test[i, :, :] = features_normalized[t:t + self.time_steps]\n",
    "            self.y_test[i] = self.targets[t + self.time_steps]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        self.X_train = torch.from_numpy(self.X_train.astype(np.float32))\n",
    "        self.y_train = torch.from_numpy(self.y_train.astype(np.float32))\n",
    "        self.X_test = torch.from_numpy(self.X_test.astype(np.float32))\n",
    "        self.y_test = torch.from_numpy(self.y_test.astype(np.float32))\n",
    "\n",
    "        print(f\"Data prepared: X_train: {self.X_train.shape}, y_train: {self.y_train.shape}\")\n",
    "        print(f\"               X_test: {self.X_test.shape}, y_test: {self.y_test.shape}\")\n",
    "\n",
    "        self.data_processed = True\n",
    "\n",
    "    def prepare_train_chunk(self, start_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Prepare a chunk of training data starting from the given index\n",
    "        end_idx = min(start_idx + self.trainer_chunk_size, len(self.X_train))\n",
    "        X_chunk = self.X_train[start_idx:end_idx]\n",
    "        y_chunk = self.y_train[start_idx:end_idx]\n",
    "        return X_chunk, y_chunk\n",
    "\n",
    "    def prepare_inference_chunk(self, start_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Prepare a chunk of inference data starting from the given index\n",
    "        end_idx = min(start_idx + self.inference_chunk_size, len(self.X_test))\n",
    "        X_chunk = self.X_test[start_idx:end_idx]\n",
    "        y_chunk = self.y_test[start_idx:end_idx]\n",
    "        return X_chunk, y_chunk\n",
    "\n",
    "    def send_to_trainer(self, data_chunk: Tuple[torch.Tensor, torch.Tensor]) -> None:\n",
    "        # Send a chunk of data to the trainer through the thread-safe buffer\n",
    "        self.trainer_empty.acquire()\n",
    "        self.trainer_mutex.acquire()\n",
    "\n",
    "        self.trainer_buffer.put(data_chunk)\n",
    "        self.trainer_chunks_sent += 1\n",
    "        print(f\"Sent chunk {self.trainer_chunks_sent} to trainer\")\n",
    "\n",
    "        self.trainer_mutex.release()\n",
    "        self.trainer_full.release()\n",
    "\n",
    "    def send_to_inferer(self, data_chunk: Tuple[torch.Tensor, torch.Tensor]) -> None:\n",
    "        # Send a chunk of data to the inference through the thread-safe buffer\n",
    "        self.inference_empty.acquire()\n",
    "        self.inference_mutex.acquire()\n",
    "\n",
    "        self.inference_buffer.put(data_chunk)\n",
    "        self.inference_chunks_sent += 1\n",
    "        print(f\"Sent chunk {self.inference_chunks_sent} to inference\")\n",
    "\n",
    "        self.inference_mutex.release()\n",
    "        self.inference_full.release()\n",
    "\n",
    "    def run(self) -> None:\n",
    "        print(\"Starting data preprocessing thread\")\n",
    "\n",
    "        # Load and prepare data\n",
    "        if self.data is None:\n",
    "            self.load_data()\n",
    "\n",
    "        if not self.features_selected:\n",
    "            self.select_features()\n",
    "\n",
    "        if not self.data_processed:\n",
    "            self.prepare_data()\n",
    "\n",
    "        # Send training data chunks\n",
    "        start_time = time.time()\n",
    "        train_idx = 0\n",
    "        test_idx = 0\n",
    "\n",
    "        # Process all training data first\n",
    "        while train_idx < len(self.X_train):\n",
    "            # Prepare and send training data chunk\n",
    "            train_chunk = self.prepare_train_chunk(train_idx)\n",
    "            self.send_to_trainer(train_chunk)\n",
    "            train_idx += self.trainer_chunk_size\n",
    "\n",
    "            # Simulate processing time\n",
    "            time.sleep(0.5)  # Adjust this time as needed\n",
    "\n",
    "        # Then process test data for inference\n",
    "        while test_idx < len(self.X_test):\n",
    "            # Prepare and send inference data chunk\n",
    "            inference_chunk = self.prepare_inference_chunk(test_idx)\n",
    "            self.send_to_inferer(inference_chunk)\n",
    "            test_idx += self.inference_chunk_size\n",
    "\n",
    "            # Simulate processing time\n",
    "            time.sleep(0.2)  # Adjust this time as needed\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Data preprocessing completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Sent {self.trainer_chunks_sent} chunks to trainer and {self.inference_chunks_sent} chunks to inferer\")\n",
    "\n",
    "        # Signal that we're done feeding data\n",
    "        self.is_running = False\n",
    "\n",
    "    def get_buffer_status(self) -> Dict[str, int]:\n",
    "        # Get the current status of the buffers\n",
    "        return {\n",
    "            'trainer_buffer_size': self.trainer_buffer.qsize(),\n",
    "            'inference_buffer_size': self.inference_buffer.qsize(),\n",
    "            'trainer_chunks_sent': self.trainer_chunks_sent,\n",
    "            'inference_chunks_sent': self.inference_chunks_sent\n",
    "        }\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        # Stop the preprocessing thread\n",
    "        self.is_running = False"
   ],
   "id": "42cbb830b7888096",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "488264934ec48caf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
